# Code Quality

Quality philosophy, tooling, and enforcement for the collector service. For pipeline-wide quality standards, see the [system Code Quality page](https://github.com/couchcryptid/storm-data-system/wiki/Code-Quality).

## Coding Philosophy

### Type safety as a guardrail

TypeScript strict mode catches an entire class of bugs at compile time. Zod validates configuration at startup, ensuring runtime values match expected shapes before any processing begins. The type system is a collaborator, not an obstacle.

### Structured, composable modules

Each concern has a clear home: CSV parsing (`csv/`), Kafka publishing (`kafka/`), scheduling (`scheduler/`), health and metrics (`health/`, `metrics.ts`). Modules export narrow interfaces and accept dependencies via constructor parameters.

### Fail fast on bad configuration

Configuration is validated with Zod at startup. Missing or invalid environment variables cause an immediate exit with a descriptive error -- not a cryptic runtime failure minutes later.

### Batch-oriented processing

CSV rows are accumulated into batches before publishing to Kafka. This amortizes network overhead and aligns with Kafka's strength as a batch-friendly transport. Batch size and flush intervals are configurable.

### Structured logging

All log calls use Pino with structured context as the first argument. In development, `pino-pretty` provides human-readable output. In production, logs are emitted as JSON for machine parsing. Log levels are configurable via `LOG_LEVEL`.

### Cron schedule matches data cadence

NOAA publishes daily CSV files. The collector fetches on a matching schedule (configurable cron expression) rather than polling in real time. It also runs once immediately on startup for fast feedback during development.

## What the Codebase Reveals

The collector's code reveals a service deliberately scoped to do one thing well: transport data without interpreting it.

### The collector is a transport, not a processor

CSV values pass through as strings. No numeric parsing, no date conversion, no magnitude interpretation. The only transformation is normalizing `torn` to `tornado` in the type field. This means the collector never needs to change when downstream parsing rules evolve -- it's insulated from the ETL's business logic by design. Changes to how hail sizes or wind speeds are interpreted happen in the ETL, not here.

### TypeScript strict mode does the compiler's job

Zod validates configuration at startup. TypeScript strict mode catches null/undefined access at compile time. The combination means runtime errors from bad config or type mismatches are effectively eliminated. Changes to configuration (new env vars, new defaults) are validated by the type system before any code runs.

### Concurrency is the default, not an optimization

`Promise.allSettled()` fetches all three CSV types concurrently. If one type 404s (not published yet), the others succeed independently. This isn't an optimization for speed -- it's an isolation pattern. Changes to one report type's availability or format don't cascade to the others.

### The retry strategy matches the failure mode

HTTP fetches use fixed 5-minute intervals (NOAA outages are typically short). Kafka publishes use exponential backoff (broker elections resolve in seconds). Different failure modes get different retry strategies rather than a one-size-fits-all approach. Changes to retry behavior are localized to the specific failure domain.

## Static Analysis

### ESLint + TypeScript

ESLint flat config with `@typescript-eslint` provides type-aware linting. Key checks:

| Category | What It Catches |
|----------|----------------|
| Type safety | `@typescript-eslint/recommended-type-checked` rules |
| Unused code | Unused variables, imports, parameters |
| Style | Consistent patterns via ESLint recommended rules |
| Formatting | Prettier integration (no conflicts between linter and formatter) |

TypeScript compiler (`tsc --noEmit`) runs as a separate CI step to verify full type safety.

### Prettier

All TypeScript files are formatted with Prettier. Configuration enforced via:
- Pre-commit hook (`prettier --write` on staged files)
- CI lint job (`npm run lint`)

### SonarQube Cloud

Analyzed via CI on every push and pull request: [SonarCloud dashboard](https://sonarcloud.io/summary/overall?id=couchcryptid_storm-data-collector)

SonarCloud configuration (`sonar-project.properties`):

- Reports TypeScript coverage via `coverage/lcov.info` (generated by `@vitest/coverage-v8`)

## Security

| Layer | What It Catches |
|-------|----------------|
| TypeScript strict mode | Null/undefined access, type mismatches |
| `@typescript-eslint` | Type-aware pattern detection |
| Zod config validation | Malformed configuration at startup |
| SonarCloud hotspots | Framework-specific security patterns |

## Quality Gates

### Pre-commit Hooks

Husky + lint-staged runs on every commit:

1. `prettier --write` on staged TypeScript files
2. `eslint --fix` on staged TypeScript files
3. Full unit test suite (`npm run test:unit`)

Integration tests are excluded from pre-commit (too slow). Run them manually before PRs with `npm run test:integration`.

### CI Pipeline

Every push and pull request to `main` runs:

| Job | Command | What It Enforces |
|-----|---------|-----------------|
| `test-unit` | `npm run test:unit` | Unit tests |
| `lint` | `npm run lint` + `npm run typecheck` | ESLint + TypeScript strict mode |
| `build` | `npm run build` | Compile check |
| `sonarcloud` | SonarCloud scan | Coverage, bugs, vulnerabilities, code smells, security hotspots |

### SonarCloud Quality Gate

Uses the default "Sonar way" gate on new code: >= 80% coverage, <= 3% duplication, A ratings for reliability/security/maintainability, 100% security hotspots reviewed.

## Testing

Tests are organized in two tiers. See [[Development]] for commands.

| Tier | What It Covers | Docker |
|------|---------------|--------|
| Unit | CSV parsing, scheduling, Kafka publishing, metrics (mocked dependencies) | No |
| Integration | Full CSV-to-Kafka pipeline with real Kafka (testcontainers) | Yes |

Unit tests use Vitest with `@vitest/coverage-v8` for coverage reporting (V8-native coverage, not Istanbul).

## Related

- [System Code Quality](https://github.com/couchcryptid/storm-data-system/wiki/Code-Quality) -- pipeline-wide quality standards
- [[Development]] -- commands, CI pipeline, logging, metrics
- [[Architecture]] -- service design, module layout
